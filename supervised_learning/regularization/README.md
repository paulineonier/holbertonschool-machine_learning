ðŸ“Œ Fiche 1 â€” Regularization (RÃ©gularisation)

ðŸ”¹ DÃ©finition
La rÃ©gularisation est un ensemble de techniques utilisÃ©es pour rÃ©duire le surapprentissage (overfitting) dâ€™un modÃ¨le.
Elle agit en contraignant la complexitÃ© du modÃ¨le (rÃ©duire ses degrÃ©s de libertÃ©, sa capacitÃ© Ã  mÃ©moriser les donnÃ©es dâ€™entraÃ®nement au lieu de gÃ©nÃ©raliser).

ðŸ”¹ But
AmÃ©liorer la gÃ©nÃ©ralisation : bonnes performances non seulement sur lâ€™entraÃ®nement, mais aussi sur la validation/test.
EmpÃªcher le modÃ¨le dâ€™apprendre trop de bruit ou de particularitÃ©s spÃ©cifiques aux donnÃ©es dâ€™entraÃ®nement.

ðŸ“Œ Fiche 2 â€” L1 et L2 Regularization

ðŸ”¹ Principe gÃ©nÃ©ral
On ajoute une pÃ©nalitÃ© sur les poids ww du modÃ¨le dans la fonction de coÃ»t :

Lossreg=Lossoriginal+Î»â‹…Penalty(w)

ðŸ”¹ L1 (Lasso regularization)
PÃ©nalitÃ© :

PenaltyL1=âˆ‘âˆ£wiâˆ£

Effet :
Encourage les poids Ã  devenir exactement 0 â†’ favorise la sparsitÃ© (sÃ©lection de variables).
Utile pour rÃ©duire la dimensionnalitÃ© et faire du feature selection.

ðŸ”¹ L2 (Ridge regularization)
PÃ©nalitÃ© : PenaltyL2=âˆ‘wi2
Effet :
RÃ©duit lâ€™amplitude des poids mais rarement Ã  0.
Encourage des poids petits et rÃ©partis.
Plus stable numÃ©riquement que L1.

ðŸ”¹ DiffÃ©rences clÃ©s
L1 : produit des poids exactement nuls â†’ sÃ©lection de variables.
L2 : rÃ©duit les poids mais les garde non nuls â†’ meilleure stabilitÃ©.
Elastic Net : combinaison des deux.

ðŸ“Œ Fiche 3 â€” Dropout

ðŸ”¹ DÃ©finition
Technique de rÃ©gularisation pour rÃ©seaux de neurones.
Pendant lâ€™entraÃ®nement, on dÃ©sactive alÃ©atoirement un certain pourcentage de neurones (ex. 50%).

ðŸ”¹ But
Ã‰vite que le rÃ©seau devienne trop dÃ©pendant dâ€™un petit ensemble de neurones.
Force le modÃ¨le Ã  apprendre des reprÃ©sentations redondantes et robustes.

ðŸ”¹ Remarque
UtilisÃ© seulement Ã  lâ€™entraÃ®nement.
En test/infÃ©rence, on garde tous les neurones mais on rÃ©duit leurs poids par le taux de dropout.

ðŸ“Œ Fiche 4 â€” Early Stopping

ðŸ”¹ DÃ©finition
StratÃ©gie qui consiste Ã  arrÃªter lâ€™entraÃ®nement automatiquement quand la performance sur les donnÃ©es de validation commence Ã  se dÃ©grader.

ðŸ”¹ But
Ã‰viter que le modÃ¨le continue Ã  apprendre le bruit (overfitting).

ðŸ”¹ Exemple
On suit la val_loss (erreur sur la validation).
Si elle nâ€™amÃ©liore plus aprÃ¨s N itÃ©rations â†’ on stoppe lâ€™entraÃ®nement.

ðŸ“Œ Fiche 5 â€” Data Augmentation

ðŸ”¹ DÃ©finition
Technique pour crÃ©er artificiellement plus de donnÃ©es dâ€™entraÃ®nement Ã  partir des donnÃ©es existantes.

ðŸ”¹ Exemples en vision
Rotation, recadrage, zoom, bruit, inversion horizontale, changement de luminositÃ©.

ðŸ”¹ Exemples en NLP
Synonym replacement, back-translation.

ðŸ”¹ But
RÃ©duire le surapprentissage.
AmÃ©liorer la robustesse.



ðŸ“Œ Fiche 6 â€” ImplÃ©mentation (Numpy vs TensorFlow)

ðŸ”¹ L1 et L2
Numpy :
import numpy as np

def l1_penalty(w, lambd=0.01):
    return lambd * np.sum(np.abs(w))

def l2_penalty(w, lambd=0.01):
    return lambd * np.sum(w**2)

TensorFlow/Keras :
from tensorflow.keras import regularizers

# L1
Dense(64, activation='relu', kernel_regularizer=regularizers.l1(0.01))

# L2
Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))

# L1 + L2 (Elastic Net)
Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(0.01, 0.01))


ðŸ”¹ Dropout
Numpy (simulÃ©) :
def dropout_layer(X, drop_rate=0.5):
    mask = (np.random.rand(*X.shape) > drop_rate).astype(float)
    return (X * mask) / (1 - drop_rate)

TensorFlow/Keras :
from tensorflow.keras.layers import Dropout

model.add(Dropout(0.5))


ðŸ”¹ Early Stopping
Numpy : implÃ©mentation maison â†’ surveiller la val_loss, arrÃªter si elle nâ€™amÃ©liore pas.
TensorFlow/Keras :
from tensorflow.keras.callbacks import EarlyStopping

early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

ðŸ”¹ Data Augmentation
Numpy : Ã©crit Ã  la main (rotations, bruit, flips, etc.).
TensorFlow/Keras (vision) :
from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)
